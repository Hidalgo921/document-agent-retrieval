{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_dotenv()\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_CLOUD_API_KEY')\n",
    "OPENAI_KEY = os.getenv('OPEN_AI_API_KEY')\n",
    "HF_KEY = os.getenv('HF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_key= OPENAI_KEY)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "#     \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "#     \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "# ]\n",
    "\n",
    "# struc_doc_urls =[\n",
    "#     \"https://www.opm.gov/forms/pdfimage/sf50.pdf\"\n",
    "# ]\n",
    "\n",
    "# files = []\n",
    "\n",
    "# all_urls = urls + struc_doc_urls\n",
    "# for url in all_urls:\n",
    "#     file = wget.download(url=url)\n",
    "#     files.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def parse_image(file_path):\n",
    "    image = Image.open(file_path)\n",
    "    text = LlamaParse.parse_image(image)\n",
    "    return text\n",
    "\n",
    "def base_reader(file_path):\n",
    "    reader = LlamaParse(result_type = 'text')\n",
    "    base_docs = reader.load_data(file_path)\n",
    "    return base_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"travel_doc_sample2.pdf\",\n",
    "    \"travel_doc_sample.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "def _load(file_path: str) -> Document:\n",
    "    parser = LlamaParse(result_type='text')\n",
    "    json_response = parser.get_json_result(file_path)\n",
    "    page_results = json_response[0]['pages']\n",
    "    docs = []\n",
    "    for item in page_results:\n",
    "        doc = Document(\n",
    "                text=item.get(\"text\"), \n",
    "                metadata={\"page_content\":item[\"page\"]}\n",
    "            )\n",
    "    docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, LlamaParseJsonNodeParser\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def get_vec_tools(file_path:str) -> str:\n",
    "    docs = _load(file_path)\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "    doc_index = VectorStoreIndex(nodes = nodes)\n",
    "\n",
    "    query_engine = doc_index.as_query_engine(similarity_top_k=5, verbose=True)\n",
    "\n",
    "    name = Path(file_path).stem.split(\" \")[0]\n",
    "    query_tool = QueryEngineTool(\n",
    "        query_engine = query_engine,\n",
    "        metadata = ToolMetadata(\n",
    "            name = f'vector_tool_{name}',\n",
    "            description = (\n",
    "                f'Provides information about {name}.'\n",
    "                'Use a detailed plain text question as input to the tool.'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return query_tool\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-59de-452e-af3e-70f2663c6dbd\n",
      "Started parsing the file under job_id cac11eca-ba5c-4390-a9b8-d3a580cb3766\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for file in files:\n",
    "    file_query_tool = get_vec_tools(file)\n",
    "    paper_to_tools_dict[file] = file_query_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = ReActAgentWorker.from_tools(\n",
    "    [tool for tool in paper_to_tools_dict.values()],\n",
    "    # llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types = ['name of traveler', 'departing street address', 'dates of travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool_travel_doc_sample2\n",
      "Action Input: {'input': 'Name of traveler, departing street address, and dates of travel in travel_doc_sample2.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Johnathan Doe, 222 North St, 2/1/2024 - 2/10/2024\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The name of the traveler is Johnathan Doe, the departing street address is 222 North St, and the dates of travel are from 2/1/2024 to 2/10/2024.\n",
      "\u001b[0mThe name of the traveler is Johnathan Doe, the departing street address is 222 North St, and the dates of travel are from 2/1/2024 to 2/10/2024.\n",
      "\u001b[1;3;38;5;200mThought: The user is asking for specific information from a travel document. I can use the tools to extract the required details.\n",
      "Action: vector_tool_travel_doc_sample\n",
      "Action Input: {'input': 'Please provide the name of the traveler, departing street address, and dates of travel from travel_doc_sample.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Johnathan Doe, 222 North St, 1/1/2024 - 1/4/2024\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have extracted the necessary information from the travel document.\n",
      "Answer: The name of the traveler is Johnathan Doe, the departing street address is 222 North St, and the dates of travel are from 1/1/2024 to 1/4/2024.\n",
      "\u001b[0mThe name of the traveler is Johnathan Doe, the departing street address is 222 North St, and the dates of travel are from 1/1/2024 to 1/4/2024.\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    prompt = f'In {file}, provide '\n",
    "    for i, question in enumerate(question_types):\n",
    "        if i == len(question_types)-1:\n",
    "            prompt += 'and ' + question\n",
    "        else:\n",
    "            prompt += question + ', '   \n",
    "    response = agent.query(prompt)\n",
    "    print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
